# Proofread of Computing Note

1
Linear regression: ~~least squares~~ Least Squares, ~~ridge~~ Ridge, Lasso

1.1
Computationally, the above property enables us to implement the matrix sweep ~~by~~ as a sequence of scalar sweeps.

1.2
The dataset of linear regression consists of an n × p matrix X = (xij ), and a n × 1 vector Y = (yi). The model is of the ~~following~~ form:

1.3 Gauss-Jordan elimination
For a system of linear equations Ax = b, where A = (aij) is n × n, x = (xi) is n × 1, and b = (bi) is n × 1,
we can solve for x = A^{−1} ~~by~~ using Gauss-Jordan elimination.

1.7
The Lasso regression estimate ~~by~~ is given by

1.8
The red curves ~~is~~ form the contour plot.

1.10
the algorithm maintains that <R,Xj> ~~to be~~ is lambda or -lambda

1.11
then beta1 will be ~~the~~ the intercept term.

1.10
If Y is ~~Scaler~~ Scalar
in order to maximally ~~reducing~~ reduce
